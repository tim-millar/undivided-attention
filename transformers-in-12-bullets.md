# The Transformer in 12 Bullet Points

1. The core problem the Transformer was designed to solve: 

The core problem the Transformer was designed to solve is that of sequence modelling and transduction (i.e., transferring one sequence into another, such as machine or language translation).

2. The role of self-attention and what it replaces:

Self-attention forms part of the transformer architecture and it is what allows each token token to attend to other tokens in the sequence without sequentional processing and state, allowing transformers to relpace the sequential architecture of RNNs and CNNs with parallelisable attention blocks.
